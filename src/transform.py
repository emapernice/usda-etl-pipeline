import os
import json
import pandas as pd

REQUIRED_COLUMNS = [
    "year",
    "state_name",
    "commodity_desc",
    "statisticcat_desc",
    "unit_desc",
    "value",
]


def process_raw_file(json_path):
    """
    Processes a JSON file generated by extract.py.
    Validates columns, cleans invalid data, and returns a DataFrame ready
    to be combined with other files.
    """
    try:
        with open(json_path, "r") as f:
            raw_data = json.load(f)
    except Exception as e:
        print(f"[ERROR] Could not read {json_path}: {e}")
        return pd.DataFrame()

    if not raw_data:
        print(f"[SKIP] Empty file: {json_path}")
        return pd.DataFrame()

    df = pd.DataFrame(raw_data)

    df.columns = [c.lower() for c in df.columns]

    missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]
    if missing:
        print(f"[WARNING] Missing columns in {os.path.basename(json_path)}: {missing} → Skipped")
        return pd.DataFrame()

    df = df[REQUIRED_COLUMNS].copy()

    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    df["year"] = pd.to_numeric(df["year"], errors="coerce").astype("Int64")

    df.dropna(subset=["year", "state_name", "commodity_desc", "value"], inplace=True)

    df = df[df["value"] > 0]

    df = df[df["state_name"].str.strip() != ""]

    print(f"[OK] Processed {os.path.basename(json_path)} → {len(df)} rows")
    return df


def process_all_raw(raw_folder="data/raw"):
    """
    Procesa todos los JSON válidos dentro de data/raw y genera
    un único CSV combinado en data/processed/.
    """
    if not os.path.exists(raw_folder):
        print(f"[ERROR] Folder not found: {raw_folder}")
        return pd.DataFrame()

    all_dfs = []

    for file in sorted(os.listdir(raw_folder)):
        if not file.endswith(".json"):
            continue

        file_path = os.path.join(raw_folder, file)
        df = process_raw_file(file_path)

        if not df.empty:
            all_dfs.append(df)

    if not all_dfs:
        print("[ERROR] No valid JSON files found.")
        return pd.DataFrame()

    combined = pd.concat(all_dfs, ignore_index=True)

    # Export final
    os.makedirs("data/processed", exist_ok=True)
    output_path = "data/processed/usda_processed.csv"
    combined.to_csv(output_path, index=False)

    print(f"\n[SAVED] Final dataset → {len(combined)} rows")
    print(f"[PATH] {output_path}")

    return combined


if __name__ == "__main__":
    process_all_raw()
